"""
LLM Router - Routes requests to different LLM providers
Supports: OpenAI, Anthropic, Groq, Local models
"""
import os
from typing import Dict, Any


# Supported LLM models
SUPPORTED_MODELS = {
    # OpenAI models
    'gpt-4': 'openai',
    'gpt-4-turbo': 'openai',
    'gpt-4o': 'openai',
    'gpt-3.5-turbo': 'openai',
    
    # Anthropic models
    'claude-3-opus': 'anthropic',
    'claude-3-sonnet': 'anthropic',
    'claude-3-haiku': 'anthropic',
    
    # Groq models (fast inference)
    'llama-3-70b': 'groq',
    'llama-3-8b': 'groq',
    'mixtral-8x7b': 'groq',
    
    # Local/Echo (for demo)
    'echo': 'echo',
    'local': 'local',
}


def get_provider(model_name: str) -> str:
    """
    Get the provider for a given model name
    
    Args:
        model_name: Name of the LLM model
        
    Returns:
        Provider name (openai, anthropic, groq, echo, local)
    """
    # Check exact match
    if model_name in SUPPORTED_MODELS:
        return SUPPORTED_MODELS[model_name]
    
    # Check prefix match
    for model_key, provider in SUPPORTED_MODELS.items():
        if model_name.startswith(model_key):
            return provider
    
    # Default to echo for demo
    return 'echo'


def call_llm(model_name: str, prompt: str, context: str = "") -> Dict[str, Any]:
    """
    Route LLM call to appropriate provider
    
    Args:
        model_name: Name of the LLM model to use
        prompt: User message/prompt
        context: Injected context from memory
        
    Returns:
        Dict with reply, model_used, tokens, etc.
    """
    provider = get_provider(model_name)
    
    # Build full prompt with context
    full_prompt = f"{context}\n\nUser: {prompt}" if context else prompt
    
    if provider == 'openai':
        return call_openai(model_name, full_prompt)
    elif provider == 'anthropic':
        return call_anthropic(model_name, full_prompt)
    elif provider == 'groq':
        return call_groq(model_name, full_prompt)
    elif provider == 'local':
        return call_local(model_name, full_prompt)
    else:
        # Echo mode for demo
        return call_echo(model_name, full_prompt, prompt)


def call_openai(model: str, prompt: str) -> Dict[str, Any]:
    """
    Call OpenAI API
    
    Note: Requires OPENAI_API_KEY in environment
    For demo, returns placeholder
    """
    # TODO: Implement actual OpenAI API call
    # import openai
    # response = openai.ChatCompletion.create(...)
    
    return {
        'reply': f"[OpenAI {model}] This is a placeholder response. Integrate OpenAI API for production.",
        'model_used': model,
        'provider': 'openai',
        'tokens': 0,
        'status': 'demo_mode'
    }


def call_anthropic(model: str, prompt: str) -> Dict[str, Any]:
    """
    Call Anthropic Claude API
    
    Note: Requires ANTHROPIC_API_KEY in environment
    For demo, returns placeholder
    """
    # TODO: Implement actual Anthropic API call
    # import anthropic
    # response = anthropic.Anthropic().messages.create(...)
    
    return {
        'reply': f"[Anthropic {model}] This is a placeholder response. Integrate Anthropic API for production.",
        'model_used': model,
        'provider': 'anthropic',
        'tokens': 0,
        'status': 'demo_mode'
    }


def call_groq(model: str, prompt: str) -> Dict[str, Any]:
    """
    Call Groq API (fast inference)
    
    Note: Requires GROQ_API_KEY in environment
    For demo, returns placeholder
    """
    # TODO: Implement actual Groq API call
    # from groq import Groq
    # response = Groq().chat.completions.create(...)
    
    return {
        'reply': f"[Groq {model}] This is a placeholder response. Integrate Groq API for production.",
        'model_used': model,
        'provider': 'groq',
        'tokens': 0,
        'status': 'demo_mode'
    }


def call_local(model: str, prompt: str) -> Dict[str, Any]:
    """
    Call local model (e.g., Ollama, LM Studio)
    
    Note: Requires local model server running
    For demo, returns placeholder
    """
    # TODO: Implement local model call
    # import requests
    # response = requests.post('http://localhost:11434/api/generate', ...)
    
    return {
        'reply': f"[Local {model}] This is a placeholder response. Integrate local model server for production.",
        'model_used': model,
        'provider': 'local',
        'tokens': 0,
        'status': 'demo_mode'
    }


def call_echo(model: str, full_prompt: str, original_prompt: str) -> Dict[str, Any]:
    """
    Echo mode for demo - returns formatted response showing context injection
    """
    # Extract context if present
    has_context = "User:" in full_prompt
    
    if has_context:
        context_part = full_prompt.split("User:")[0].strip()
        reply = f"[Echo Mode - {model}]\n\nâœ… Context Received ({len(context_part)} chars)\n\nðŸ“ Your message: {original_prompt}\n\nðŸ¤– Response: I received your message with injected context. In production, this would be processed by {model}."
    else:
        reply = f"[Echo Mode - {model}]\n\nðŸ“ Your message: {original_prompt}\n\nðŸ¤– Response: I received your message. In production, this would be processed by {model}."
    
    return {
        'reply': reply,
        'model_used': model,
        'provider': 'echo',
        'context_injected': has_context,
        'context_length': len(context_part) if has_context else 0,
        'tokens': len(full_prompt.split()),
        'status': 'demo_mode'
    }


def is_model_supported(model_name: str) -> bool:
    """
    Check if a model is supported
    
    Args:
        model_name: Name of the model
        
    Returns:
        True if supported, False otherwise
    """
    return model_name in SUPPORTED_MODELS or any(
        model_name.startswith(key) for key in SUPPORTED_MODELS.keys()
    )


def get_supported_models() -> Dict[str, list]:
    """
    Get list of all supported models grouped by provider
    
    Returns:
        Dict mapping provider to list of models
    """
    models_by_provider = {}
    for model, provider in SUPPORTED_MODELS.items():
        if provider not in models_by_provider:
            models_by_provider[provider] = []
        models_by_provider[provider].append(model)
    
    return models_by_provider
